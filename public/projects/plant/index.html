<!DOCTYPE html>
<html lang="en">

<head><script src="/portfolio/livereload.js?mindelay=10&amp;v=2&amp;port=45543&amp;path=portfolio/livereload" data-no-instant defer></script>
  <title>
  Object detection on different leaf types · Akhilesh Ramesh
</title>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="color-scheme" content="light dark">




<meta name="author" content="Akhilesh Ramesh">
<meta name="description" content="
   Plant Detection 


  Dataset
  
    
    Link to heading
  

This dataset was obtained from Kaggle, contained over 50k images. Moreover, for the
model training this dataset was split into training (~36k) and test images (~17k).
Kaggle Dataset

  Task
  
    
    Link to heading
  

The main task was to perform object detection to identify 38 different leaf types. The ground truth bounding boxes, which were the YOLO annotation were given in the format (centerx, centery, width, height).">
<meta name="keywords" content="">



  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Object detection on different leaf types">
  <meta name="twitter:description" content="Plant Detection Dataset Link to heading This dataset was obtained from Kaggle, contained over 50k images. Moreover, for the model training this dataset was split into training (~36k) and test images (~17k).
Kaggle Dataset
Task Link to heading The main task was to perform object detection to identify 38 different leaf types. The ground truth bounding boxes, which were the YOLO annotation were given in the format (centerx, centery, width, height).">

<meta property="og:url" content="http://localhost:45543/portfolio/projects/plant/">
  <meta property="og:site_name" content="Akhilesh Ramesh">
  <meta property="og:title" content="Object detection on different leaf types">
  <meta property="og:description" content="Plant Detection Dataset Link to heading This dataset was obtained from Kaggle, contained over 50k images. Moreover, for the model training this dataset was split into training (~36k) and test images (~17k).
Kaggle Dataset
Task Link to heading The main task was to perform object detection to identify 38 different leaf types. The ground truth bounding boxes, which were the YOLO annotation were given in the format (centerx, centery, width, height).">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="projects">
    <meta property="article:published_time" content="2026-02-10T10:00:00+01:00">
    <meta property="article:modified_time" content="2026-02-10T10:00:00+01:00">




<link rel="canonical" href="http://localhost:45543/portfolio/projects/plant/">


<link rel="preload" href="/fonts/fa-brands-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-regular-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-solid-900.woff2" as="font" type="font/woff2" crossorigin>


  
  
  <link rel="stylesheet" href="/portfolio/css/coder.css" media="screen">






  
    
    
    <link rel="stylesheet" href="/portfolio/css/coder-dark.css" media="screen">
  



 
  
    
    <link rel="stylesheet" href="/portfolio/css/custom.css" media="screen">
  





<link rel="icon" type="image/svg+xml" href="/images/favicon.svg" sizes="any">
<link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

<link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">










  
</head>






<body class="preload-transitions colorscheme-auto">
  
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


  <main class="wrapper">
    <nav class="navigation">
  <section class="container">
    
    <a class="navigation-title" href="http://localhost:45543/portfolio/">
      Akhilesh Ramesh
    </a>
    
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa-solid fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link " href="/portfolio/about/">About</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/portfolio/projects/">Projects</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


    <div class="content">
      
  <section class="container page">
  <article>
    <header>
      <h1 class="title">
        <a class="title-link" href="http://localhost:45543/portfolio/projects/plant/">
          Object detection on different leaf types
        </a>
      </h1>
    </header>

    <a href="https://github.com/AkkuRam/plant-detection">
  <i class="fab fa-fw fa-github"></i> Plant Detection 
</a>
<h2 id="dataset">
  Dataset
  <a class="heading-link" href="#dataset">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>This dataset was obtained from Kaggle, contained over 50k images. Moreover, for the
model training this dataset was split into training (~36k) and test images (~17k).</p>
<p><a href="https://www.kaggle.com/datasets/sebastianpalaciob/plantvillage-for-object-detection-yolo/data#"  class="external-link" target="_blank" rel="noopener">Kaggle Dataset</a></p>
<h2 id="task">
  Task
  <a class="heading-link" href="#task">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>The main task was to perform object detection to identify 38 different leaf types. The ground truth bounding boxes, which were the YOLO annotation were given in the format (centerx, centery, width, height).</p>
<p>In terms of preprocessing, the images were resized to 256x256 pixels, where Gaussian Blur was applied
to preprocess the salt/pepper noise present in the image. Moreover, the images were normalized as a final step.</p>
<h2 id="regressor-model">
  Regressor Model
  <a class="heading-link" href="#regressor-model">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>Here the base model used was resnet50, where this is the model for the bounding box predictions.
Where the final layer has 4 outputs representing the format (centerx, centery, width, height) representing the
bounding box.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>self<span style="color:#f92672">.</span>regressor <span style="color:#f92672">=</span> 
</span></span><span style="display:flex;"><span>nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>Linear(base_model<span style="color:#f92672">.</span>fc<span style="color:#f92672">.</span>in_features, <span style="color:#ae81ff">256</span>),
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>ReLU(),
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">128</span>),
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>ReLU(),
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">64</span>),
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>ReLU(),
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">32</span>),
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>ReLU(),  
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">4</span>),
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>Sigmoid()
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>This format (centerx, centery, width, height) is later converted into the 4 corners as the bounding box
to be displayed on the test images. This is done through the following method:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">yolo_to_xyxy</span>(box, img_w, img_h):
</span></span><span style="display:flex;"><span>    cx, cy, w, h <span style="color:#f92672">=</span> box
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    cx <span style="color:#f92672">*=</span> img_w
</span></span><span style="display:flex;"><span>    cy <span style="color:#f92672">*=</span> img_h
</span></span><span style="display:flex;"><span>    w  <span style="color:#f92672">*=</span> img_w
</span></span><span style="display:flex;"><span>    h  <span style="color:#f92672">*=</span> img_h
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    x1 <span style="color:#f92672">=</span> int(cx <span style="color:#f92672">-</span> w <span style="color:#f92672">/</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>    y1 <span style="color:#f92672">=</span> int(cy <span style="color:#f92672">-</span> h <span style="color:#f92672">/</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>    x2 <span style="color:#f92672">=</span> int(cx <span style="color:#f92672">+</span> w <span style="color:#f92672">/</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>    y2 <span style="color:#f92672">=</span> int(cy <span style="color:#f92672">+</span> h <span style="color:#f92672">/</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> x1, y1, x2, y2
</span></span></code></pre></div><p>Using the above method, the YOLO format is converted to the 4 respective corners to display the bounding boxes
as seen in the batch results section below.</p>
<h2 id="batch-results">
  Batch results
  <a class="heading-link" href="#batch-results">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<ul>
<li>Ground truth (green)</li>
<li>Model predictions (red)</li>
<li>Here 500 images were used for training and test to save computational power</li>
<li>The batch size was 8 images, where in total there were 63 total batches</li>
</ul>
<p><img src="/images/leaf_detection.png" alt="Disk"></p>
<h2 id="running-the-file">
  Running the file
  <a class="heading-link" href="#running-the-file">
    <i class="fa-solid fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<ul>
<li>To run the file execute this command in the terminal &ldquo;python -m src.train_pipeline&rdquo;</li>
</ul>

  </article>
</section>

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css"
    integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
  
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js"
    integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js"
    integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
    onload="renderMathInElement(document.body,
      {
        delimiters: [
          {left: '$$', right: '$$', display:true},
          {left: '$', right: '$', display:false},
          {left: '\\(', right: '\\)', display: false},
          {left: '\\[', right: '\\]', display: true}
        ]
      }
    );"></script>

    </div>

    <footer class="footer">
  <section class="container">
    ©
    
    2026
     Akhilesh Ramesh 
    ·
    
    Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/" target="_blank" rel="noopener">Coder</a>.
    
  </section>
</footer>

  </main>

  

  
  
  <script src="/portfolio/js/coder.js"></script>
  

  

  


  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
</body>
</html>
